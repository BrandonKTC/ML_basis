# Ridge Regression (L2 Regularization):
* regularization technique that works by helping reduce the potential for overfitting to the training data.
* it does this by adding a penalty term to the squared error that is based on the value of the coefficients.

# Lasso Regression (L1 Regularization):
* adds a penalty equal to the absolute value of the magnitude of coefficients .
** Limits the size of the coefficients.
** Can yield sparse models where some coefficients can become zero.

# Elastic Net (L1 and L2 Regularization):
seek to minimize the RSS and both the squared and absolute value terms by combining L1 + L2
* L1 constrains the sum of absolute values.
* L2 constrains the sum of squared values.

